{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e5376b1",
   "metadata": {},
   "source": [
    "### Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45492c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5e9f925-0bd9-406c-a41d-8be268260b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    load_indices_from_storage,\n",
    "    load_graph_from_storage,\n",
    ")\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, StorageContext\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.memory import ChatMemoryBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8291822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d7bc2",
   "metadata": {},
   "source": [
    "### Set embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4bc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4b82",
   "metadata": {},
   "source": [
    "### Set LLM with Ollama (tinyllama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d46a1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"tinyllama:latest\", request_timeout=120.0)\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d830ba1",
   "metadata": {},
   "source": [
    "### Test LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca746a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"Tu es un Jane Austen\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47252b4e-eb58-453c-9e7d-bab5286ee296",
   "metadata": {},
   "source": [
    "# Load stores and indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e8019-e150-4a60-b59d-cac7a4aebcba",
   "metadata": {},
   "source": [
    "### Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42c55a04-2b21-4c05-96fb-f0f9ce26bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"vector\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(\n",
    "        persist_dir=\"vector\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"vector\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274006-61a7-42d7-9556-6ddae87d2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_index = load_index_from_storage(vector_storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93585fe6",
   "metadata": {},
   "source": [
    "### Property graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"pg_graph\"),\n",
    "    graph_store=SimpleGraphStore.from_persist_dir(\n",
    "        persist_dir=\"pg_graph\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"pg_graph\"),\n",
    ")\n",
    "print(pg_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65350390",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_index = load_index_from_storage(pg_storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77873e51",
   "metadata": {},
   "source": [
    "##### Visualize property graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_index.property_graph_store.save_networkx_graph(\n",
    "    name=\"PgGraph.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a31e1f",
   "metadata": {},
   "source": [
    "# Producing analytical content with queries\n",
    "\n",
    "The following blocs of code will produce several queries and use them to generate an analytical content. Each query produces one output stored into memory. The outputs are then bundled together into final content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b17b06",
   "metadata": {},
   "source": [
    "### GraphRAG using Property Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=10000)\n",
    "chat_engine = pg_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \"\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe35f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = chat_engine.stream_chat(\"\"\"\n",
    "                                                \n",
    "                                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b2d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = memory.get_all()\n",
    "\n",
    "# Assuming chat_history is available and contains your messages\n",
    "assistant_messages = [\n",
    "    message.content \n",
    "    for message in chat_history \n",
    "    if message.role == MessageRole.ASSISTANT  # Compare with the enum directly\n",
    "]\n",
    "\n",
    "\n",
    "output_filename = r\"**/output.md\"\n",
    "# Write to a Markdown file\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for msg in assistant_messages:\n",
    "        f.write(msg + \"\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cdab09-2c63-4e23-9ca3-79102db74cf0",
   "metadata": {},
   "source": [
    "### RAG using vector store and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b852a258",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_query_template = PromptTemplate(\n",
    "    \"Nous avons fourni des informations contextuelles ci-dessous. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"En utilisant ces informations, veuillez répondre à la question suivante : {query_str}\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "add71770",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "vector_chat_engine = simple_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "       \"\"\" \n",
    "       Tu es une femme du XVIIIème siècle, tu es une écrivaine anglaise. Tu es connue pour ton roman Orgueil et Préjugés.\n",
    "       \"\"\"\n",
    "    ),\n",
    "    text_qa_template=custom_query_template,\n",
    "    verbose=False,\n",
    "    similarity_top_k=4\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream = vector_chat_engine.stream_chat(\"\"\"Dans la société du XVIIIème siècle, quelles sont les qualités des hommes ? des femmes ? \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fc419",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate = response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les chunks utilisés\n",
    "for node in response_stream.source_nodes:\n",
    "    print(node.node.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Dans la société du XVIIIème siècle, quelles sont les qualités des hommes ? des femmes ? Répond en français\"\n",
    "query_engine = simple_index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "response = query_engine.query(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39387d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les chunks utilisés\n",
    "for node in response.source_nodes:\n",
    "    print(node.node.text)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca481eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_chat_engine.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
